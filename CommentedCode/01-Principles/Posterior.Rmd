---
title: "Interpreting Bayesian Posteriors"
author: "FlorianHartig"
date: "9 May 2015"
output:
  html_document:
    keep_md: yes
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=5, fig.height=5)
```


```{r, echo=F}
set.seed(123)
library(coda)
library(IDPmisc)
```

In standard statistics, we are used to interpret search for the point that maximizes p(D|phi), and interpret this as the most likely value. 


```{r}
parameter = seq(-5,5,len=500)
likelihood = dnorm(parameter) + dnorm(parameter, mean = 2.5, sd=0.5)

plot(parameter,likelihood, type = "l")

MLEEstimate <- parameter[which.max(likelihood)]
abline(v=MLEEstimate, col = "red")
text(2.5,0.8, "MLE", col = "red")

```

Assume the prior is flat, then we get the posterior simply by normalization

```{r}
unnormalizedPosterior = likelihood * 1 
posterior = unnormalizedPosterior / sum(unnormalizedPosterior/50) 

```

In Bayesian statistics, the primary outcome of the inference is the whole distribution. 

```{r}
plot(parameter,posterior, type = "l")
polygon(parameter, posterior, border=NA, col="darksalmon")
```

If we don't have to, this is what we should interpret and forecast with. However, in many cases, people what to summarize this distribution by particular values. Here is what you typically use for different situations

### The best values 

The problem with the best values is that it depends what you want to do with it. If you want to have the most likely parameter value, what you can do is to use the mode of the posterior distribution. It is called the maximum a posteriori probability (MAP) estimate. 

However, if the distribution is very skewed as in our example, it may well be that the MAP is far at one side of the distribution, and doesn't really give a good distribution of where most probability mass is. If it is really neccessary to do predictions with one value (instead of forwarding the whole posterior distribution), I would typically predict with the median of the posterior. 


```{r}
plot(parameter,posterior, type = "l")
polygon(parameter, posterior, border=NA, col="darksalmon")


MAP <- parameter[which.max(posterior)]
abline(v=MAP, col = "red")
text(2.5,0.4, "MAP", col = "red")


medianPosterior <- parameter[min(which(cumsum(posterior) > 0.5 * 50))]
abline(v=medianPosterior, col = "blue")
text(1.8,0.3, "Median", col = "blue")
```

### Bayesian credibile intervals

Typically, one also wants uncertainties. There basic option to do this is the Bayesian credible interval, which is the analogue to the frequentist confidence interval. The 95 % Bayesian Credibility interval is the centra 95% of the posterior distribution


```{r}
plot(parameter,posterior, type = "l")


lowerCI <- min(which(cumsum(posterior) > 0.025 * 50))
upperCI <- min(which(cumsum(posterior) > 0.975 * 50))

par = parameter[c(lowerCI, lowerCI:upperCI, upperCI)]
post = c(0, posterior[lowerCI:upperCI], 0)

polygon(par, post, border=NA, col="darksalmon")

text(0.75,0.07, "95 % Credibile\n Interval")

```

There are two alternatives to the credibility interval that is particularly useful if the posterior has weird correlation structres.

1. The **Highest Posterior Density** (HPD). The HPD is the x% highest posterior density interval is the shortest interval in parameter space that contains x% of the posterior probability. It would be a bit cumbersome to calculate this in this example, but if you have an MCMC sample, you get the HPD with the package coda via

```{r, eval=FALSE}
HPDinterval(obj, prob = 0.95, ...)
```

2. The Lowest Posterior Loss (LPL) interval, which considers also the prior. 

More on both alternatives [here](http://www.bayesian-inference.com/credible)


### Multivariate issues

Things are always getting more difficult if you move to more dimensions, and Bayesian analysis is no exception. 

#### Marginal values hide correlations 

A problem that often occurs when we have more than one parameter are correlations between parameters. In this case, the marginal posterior distributions that are reported in the summary() or plot functions of coda can be VERY misleading. 

Look at the situation below, where we have two parameters that are highly correlated. The marginal posteriors look basically flat, and looking only at them you may think there is no information in the likelihood. 

However, if you look at the correlation, you see that the likelihood has excluded vast areas of the prior space (assuming we have had flat uncorrelated likelihoods in this case). 


```{r}
library(psych)
par1= runif(1000,0,1)
par2 =par1 + rnorm(1000,sd = 0.05)
scatter.hist(par1,par2)
```

It is therefore vital to plot the correlation plots as well to be able to judge the extent to which parameters are uncertaint. 

If you have more parameters, however, you may still miss things here, because there could be higher-order correlations between the parameters that look random in the two-dimensional plot. A good proxy to get an overall reduction of uncertainy across all parameters, including all these higher-order correlations, is to compare the prior predictive distribution with the posterior predictive distribution. 


#### Nonlinear correlations

A further issue that many people are not aware of is that the marginal mode (maximum) does not need to coincide with the global mode if correlations in parameter space are nonlinear. Assume we have a posterior with 2 parameters, which are in a complcated, banana-shaped correlation. Assume we are able to sample from this poterior. Here is an example from Meng and Barnard, code from the bayesm package (see Rmd source file for code of this function).

```{r, echo=F}
banana=function(A,B,C1,C2,N,keep=10,init=10)
{
    R=init*keep+N*keep
    x1=x2=0
    bimat=matrix(double(2*N),ncol=2)
    for (r in 1:R) {
        x1=rnorm(1,mean=(B*x2+C1)/(A*(x2^2)+1),sd=sqrt(1/(A*(x2^2)+1)))
        x2=rnorm(1,mean=(B*x2+C2)/(A*(x1^2)+1),sd=sqrt(1/(A*(x1^2)+1)))
        if (r>init*keep && r%%keep==0) {
            mkeep=r/keep; bimat[mkeep-init,]=c(x1,x2)
        }
    }

    return(bimat)
}

scatterhist = function (x, y = NULL, smooth = TRUE, ab = FALSE, correl = TRUE, 
    density = TRUE, ellipse = TRUE, digits = 2, method, cex.cor = 1, 
    title = "Scatter plot + histograms", xlab = NULL, ylab = NULL, smoothScatterPlot = T, histBreaks = 50,
    ...) 
{
    old.par <- par(no.readonly = TRUE)
    if (missing(xlab)) {
        if (!is.null(colnames(x))) {
            xlab = colnames(x)[1]
            ylab = colnames(x)[2]
        }
        else {
            xlab = "V1"
            ylab = "V2"
        }
    }
    if (is.null(y)) {
        y <- x[, 2]
        x <- x[, 1]
    }
    else {
        if (!is.null(dim(x))) {
            x <- x[, 1, drop = TRUE]
            if (!is.null(colnames(y))) 
                ylab <- colnames(y)
            if (!is.null(dim(y))) {
                y <- y[, 1, drop = TRUE]
            }
        }
    }
    xhist <- hist(x, breaks = histBreaks, plot = FALSE)
    yhist <- hist(y, breaks = histBreaks, plot = FALSE)
    xrange <- range(x, na.rm = TRUE)
    yrange <- range(y, na.rm = TRUE)
    nf <- layout(matrix(c(2, 4, 1, 3), 2, 2, byrow = TRUE), c(3, 
        1), c(1, 3), TRUE)
    par(mar = c(5, 4, 1, 1))
    
    
    if (smoothScatterPlot == T) smoothScatter(x,y, colramp = colorRampPalette(c("white", "darkorange", "darkred", "darkslateblue")), xlim = xrange, ylim = yrange, xlab = xlab, ylab = ylab, ...)
    else plot(x, y, xlim = xrange, ylim = yrange, xlab = xlab, ylab = ylab, ...)
    
    
    if (ab) 
        abline(lm(y ~ x))
    if (smooth) {
        ok <- is.finite(x) & is.finite(y)
        if (any(ok)) 
            lines(stats::lowess(x[ok], y[ok]), col = "red")
    }
    if (ellipse) {
        ellipses(x, y, add = TRUE)
    }
    par(mar = c(0, 4, 2, 0))
    mp <- barplot(xhist$density, axes = FALSE, space = 0)
    tryd <- try(d <- density(x, na.rm = TRUE, bw = "nrd", adjust = 1.2), 
        silent = TRUE)
    if (class(tryd) != "try-error") {
        d$x <- (mp[length(mp)] - mp[1] + 1) * (d$x - min(xhist$breaks))/(max(xhist$breaks) - 
            min(xhist$breaks))
        if (density) 
            lines(d)
    }
    title(title)
    par(mar = c(5, 0, 0, 2))
    mp <- barplot(yhist$density, axes = FALSE, space = 0, horiz = TRUE)
    tryd <- try(d <- density(y, na.rm = TRUE, bw = "nrd", adjust = 1.2), 
        silent = TRUE)
    if (class(tryd) != "try-error") {
        temp <- d$y
        d$y <- (mp[length(mp)] - mp[1] + 1) * (d$x - min(yhist$breaks))/(max(yhist$breaks) - 
            min(yhist$breaks))
        d$x <- temp
        if (density) 
            lines(d)
    }
    par(mar = c(3, 1, 1, 1))
    if (correl) {
        plot(1, 1, type = "n", axes = FALSE)
        med.x <- median(x, na.rm = TRUE)
        med.y <- median(y, na.rm = TRUE)
        if (missing(method)) 
            method <- "pearson"
        r = (cor(x, y, use = "pairwise", method = method))
        txt <- format(c(r, 0.123456789), digits = digits)[1]
        if (missing(cex.cor)) {
            cex <- 0.75/strwidth(txt)
        }
        else {
            cex <- cex.cor
        }
        text(1, 1, txt, cex = cex)
    }
    par(old.par)
}

```

If we plot the correlation, as well as the marginal distributions (i.e. the histograms for each parameter), you see that the mode of the marginal distributions will not conincide with the multivariate mode (red, solid lines).

```{r, fig.width=8, fig.height=8}
set.seed(124)
sample=banana(A=0.5,B=0,C1=3,C2=3,50000)
scatterhist(sample[,1], sample[,2])

#par(mfg = c(2,1))

# skaliert nicht richtig!!!, vielleicht in die Histogramme einbauen?

#abline(h = 0.22, col = "green", lwd = 3, lty =2)
#abline(v = 0.295, col = "green", lwd = 3, lty =2)

```

Hence, it's important to note that the marginal distributions are not suited to calculate the MAP, CIs, HPDs or any other summary statistics if the posterior distribution is not symmetric in multivariate space. This is a real point of confusion for many people, so keep it in mind!

More options to plot HPD in 2-d here http://www.sumsar.net/blog/2014/11/how-to-summarize-a-2d-posterior-using-a-highest-density-ellipse/



### FAQs

* http://stats.stackexchange.com/questions/176436/what-would-be-the-reason-that-the-posterior-distribution-looks-like-the-prior-us




---
**Copyright, reuse and updates**: By Florian Hartig. Updates will be posted at https://github.com/florianhartig/LearningBayes. Reuse permitted under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License

